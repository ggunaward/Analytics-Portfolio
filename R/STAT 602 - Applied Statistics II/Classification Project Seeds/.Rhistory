usePackage("rstatix")
usePackage("DT")
#Import data from csv file
SeedDataInitial.df <- as.data.frame(read.csv("labeled.csv"))
#Confirm data wasimported
head(SeedDataInitial.df)
#Create dataframes with necessary key information for calculating bean value and cost of error
#Create dataframe to store bean data
BeanKey.df <- data.frame (ClassID = factor(seq(1,6,1)),
Class  = c("Bombay", "Cali", "Dermason", "Horoz", "Seker", "Sira"),
dollars_per_lb = c(5.56,6.02,1.98,2.43,2.72,5.4),
grams_per_seed = c(1.92,.61,.28,.52,.49,.38)
)
BeanKey.df$Class <- toupper(BeanKey.df$Class)
attach(BeanKey.df)
BeanKey.df$approx_lbs_per_seed <- round(grams_per_seed/453.592,4)
attach(BeanKey.df)
BeanKey.df$approx_dollars_per_seed <- round(approx_lbs_per_seed * dollars_per_lb,4)
attach(BeanKey.df)
BeanKey.df$approx_dollars_per_gram <- round(dollars_per_lb / 453.592,4)
#Create dataframe to store descriptions of the independent variables of the bean data
bean_column_descriptions.df <- data.frame(ID = seq(1,7,1),
name = c("Area","Perimeter", "MajorAxisLength", "MinorAxisLength", "Eccentricity", "ConvexArea", "Extent"),
# abbreviation = c("A","P","L","I","Ec","C", "Ex"),
description = c("The area of a bean zone and the number of pixels within its boundaries.","Bean circumference is defined as the length of its border.","The distance between the ends of the longest line that can be drawn from a bean.","The longest line that can be drawn from the bean while standing perpendicular to the main axis.","Eccentricity of the ellipse having the same moments as the region.","Number of pixels in the smallest convex polygon that can contain the area of a bean seed.","The ratio of the pixels in the bounding box to the bean area.")
)
#Create functions to calculate bean data
#Calculate weight of a sample of beans in grams
get.Bean.Weight <- function(BeanList, BeanKey){
temp.df <- left_join(BeanList, BeanKey, by='ClassID')
return(sum(temp.df$grams_per_seed, na.rm = TRUE))
}
#Calculate cost of a sample of beans
get.Bean.Cost <- function(BeanList, BeanKey){
temp.df <- left_join(BeanList, BeanKey, by='ClassID')
return(sum(temp.df$approx_dollars_per_seed, na.rm = TRUE))
}
#Compare weights of 2 samples of beans
compare.Bean.Weight <- function(BeanListActuals, BeanListPredicted, BeanKey){
beansActualWeight <- get.Bean.Weight(BeanListActuals, BeanKey)
beansPredictedWeight <- get.Bean.Weight(BeanListPredicted, BeanKey)
return(abs(beansActualWeight - beansPredictedWeight))
}
#Compare costs of 2 samples of beans
compare.Bean.Cost <- function(BeanListActuals, BeanListPredicted, BeanKey){
beansActualCost <- get.Bean.Cost(BeanListActuals, BeanKey)
beansPredictedCost <- get.Bean.Cost(BeanListPredicted, BeanKey)
return(abs(beansActualCost - beansPredictedCost))
}
# Create functions to calculate Z Scores, Skewness, and Kurtosis
# Z score function
ZScore.Solve <- function(x.dat, y_hat = mean(x.dat), s_dev = sd(x.dat)) {
z_score <- (x.dat - y_hat)/sqrt(s_dev)
return(z_score)
}
#Calculate Skewness function
Skewness.fun <- function(arrayOfValues){
mean.x <- 0
stDev.x <- 0
sum.x <- 0
skewness.x <- 0
kurtosis.x <- 0
runningCalcTotal1.x <- 0 #standard deviation
runningCalcTotal2.x <- 0 #skewness
runningCalcTotal3.x <- 0 #kurtosis
n <- 0
#Mean calculation
for (i in 1:length(arrayOfValues)){
if(!is.na(arrayOfValues[i])){
sum.x <- sum.x + arrayOfValues[i]
n <- n+1
}
}
mean.x <- sum.x/n
#Standard Deviation Calculation
for (i in 1:length(arrayOfValues)){
if(!is.na(arrayOfValues[i])){
runningCalcTotal1.x <- runningCalcTotal1.x + ((arrayOfValues[i]-mean.x) * (arrayOfValues[i]-mean.x))
}
}
stDev.x <- sqrt(runningCalcTotal1.x/(n-1))
#Fisher-Pearson  skewness Calculation
for (i in 1:length(arrayOfValues)){
if(!is.na(arrayOfValues[i])){
runningCalcTotal2.x <- (runningCalcTotal2.x + ((arrayOfValues[i]-mean.x) * (arrayOfValues[i]-mean.x)^2))
}
}
skewness.x <- ((runningCalcTotal2.x/n) / stDev.x^3)
return(skewness.x)
}
#Calculate Kurtosis function
Kurtosis.fun <- function(arrayOfValues){
mean.x <- 0
stDev.x <- 0
sum.x <- 0
skewness.x <- 0
kurtosis.x <- 0
runningCalcTotal1.x <- 0 #standard deviation
runningCalcTotal2.x <- 0 #skewness
runningCalcTotal3.x <- 0 #kurtosis
n <- 0
#Mean calculation
for (i in 1:length(arrayOfValues)){
if(!is.na(arrayOfValues[i])){
sum.x <- sum.x + arrayOfValues[i]
n <- n+1
}
}
mean.x <- sum.x/n
#Standard Deviation Calculation
for (i in 1:length(arrayOfValues)){
if(!is.na(arrayOfValues[i])){
runningCalcTotal1.x <- runningCalcTotal1.x + ((arrayOfValues[i]-mean.x) * (arrayOfValues[i]-mean.x))
}
}
stDev.x <- sqrt(runningCalcTotal1.x/(n-1))
#kurtosis Calculation
for (i in 1:length(arrayOfValues)){
if(!is.na(arrayOfValues[i])){
runningCalcTotal3.x <- (runningCalcTotal3.x + ((arrayOfValues[i]-mean.x) * (arrayOfValues[i]-mean.x)^3))
}
}
kurtosis.x <- ((runningCalcTotal3.x/n) / stDev.x^4)
return(kurtosis.x)
}
#Functions to convert Class Variable to and from a numeric ID
#Convert class names to class IDs and return all dependent and independent variables
convert.class.to.classID <- function(BeanData.df, BeanKey=BeanKey.df){
temp.1.df <- left_join(BeanData.df, BeanKey.df, by='Class')
return(subset(temp.1.df, select = c("Area","Perimeter","MajorAxisLength", "MinorAxisLength", "Eccentricity", "ConvexArea","Extent", "ClassID") ))
}
#Convert class ID to class names and return all dependent and independent variables
convert.classID.to.class <- function(BeanData.df, BeanKey=BeanKey.df){
temp.1.df <- left_join(BeanData.df, BeanKey.df, by='ClassID')
return(subset(temp.1.df, select = c("Area","Perimeter","MajorAxisLength", "MinorAxisLength", "Eccentricity", "ConvexArea","Extent", "Class") ))
}
#Convert class IDs to Class names and return the class names
convert.classID.to.class.2 <- function(BeanData.df, BeanKey=BeanKey.df){
temp.1.df <- left_join(BeanData.df, BeanKey.df, by='ClassID')
return(subset(temp.1.df, select = c("Class") ))
}
#Convert a factor of class IDs to Class Names and return the class names as a factor
convert.classID.to.class.2.factor <- function(BeanData, BeanKey=BeanKey.df){
tempBeanDf <- data.frame(ClassID = BeanData)
temp.1.df <- left_join(tempBeanDf, BeanKey.df, by='ClassID')
result <- factor(subset(temp.1.df, select = Class )$Class, levels = c("BOMBAY", "CALI", "DERMASON", "HOROZ", "SEKER", "SIRA"))
return(result)
}
#Create function to remove outliers
remove_outliers_beans_dataset <- function(x.df){
#Create temporary dataset with row numbers as a column
Data.All.df.No.Outliers.Temp <- x.df
Data.All.df.No.Outliers.Temp$row_names <- row.names(Data.All.df.No.Outliers.Temp)
Dependent.Var.Columns <-c("Area","Perimeter","MajorAxisLength", "MinorAxisLength", "Eccentricity", "ConvexArea","Extent")
for (i in Dependent.Var.Columns) {
#Identify the values in each aliquot that are outliers
Data.All.df.No.Outliers.Temp2 <- Data.All.df.No.Outliers.Temp %>%
group_by(Class) %>%
identify_outliers(all_of(i))
#Use recursion to rerun the function for identifying and removing outliers in case there are any left after each run
while (length(Data.All.df.No.Outliers.Temp2[,all_of(i)]) > 0)
{
# create a list of all row numbers that are outliers
list.outliers <- as.list(Data.All.df.No.Outliers.Temp2$row_names)
# Remove the outliers from the main dataset based on the row numbers of outliers list
Data.All.df.No.Outliers.Temp <- Data.All.df.No.Outliers.Temp[!Data.All.df.No.Outliers.Temp$row_names %in% list.outliers,]
#Identify the values in each aliquot that are outliers
Data.All.df.No.Outliers.Temp2 <- Data.All.df.No.Outliers.Temp %>%
group_by(Class) %>%
identify_outliers(all_of(i))
}
}
return(Data.All.df.No.Outliers.Temp)
}
#Display some of the created dataframes
print(BeanKey.df)
print(bean_column_descriptions.df)
#Check distribution between classes and look for class imbalance
#https://www.statology.org/r-frequency-table-by-group/
# import_plex_sans()
beanCounts <- SeedDataInitial.df %>%
group_by(Class) %>%
summarize(Freq=n())
print(beanCounts)
#Check distribution of data within classes and look for outliers that could be the result of an error
#https://r-graph-gallery.com/histogram_several_group.html
#https://cmdlinetips.com/2019/02/how-to-make-grouped-boxplots-with-ggplot2/
#Convert data to long format
SeedDataLong.df <- SeedDataInitial.df %>%                                   # Apply pivot_longer function
pivot_longer(c("Area","Perimeter", "MajorAxisLength", "MinorAxisLength", "Eccentricity", "ConvexArea", "Extent"), names_to = "variable")
#Plots
#Box plots of the dataset
for (var_ in unique(SeedDataLong.df$variable)) {
plotValue <- SeedDataLong.df %>%
filter(variable %in% var_) %>%
ggplot(aes(x=Class, y=value, fill=Class)) +
geom_boxplot() +
stat_boxplot(geom ='errorbar', width = 0.6) +
geom_jitter(width=0.1,alpha=0.2) +
labs(y=var_)
print(plotValue)
}
#Histograms of the dataset
for (var_ in unique(SeedDataLong.df$variable)) {
plotValue2 <- SeedDataLong.df %>%
filter(variable %in% var_) %>%
ggplot( aes(x=value, color=Class, fill=Class)) +
geom_histogram(alpha=0.6) +
scale_fill_viridis(discrete=TRUE) +
scale_color_viridis(discrete=TRUE) +
# theme_ipsum() +
theme(
legend.position="none",
panel.spacing = unit(0.1, "lines"),
axis.text.x = element_text(angle=45)
) +
facet_wrap(~Class)+
labs(subtitle=var_)
print(plotValue2)
}
#View Kurtosis and Skewness of the dataset
Class_val <- list()
Variable_val <- list()
Skewness_val <- list()
Kurtosis_val <- list()
i <- 1
for (bean_ in unique(SeedDataLong.df$Class)){
for (var_ in unique(SeedDataLong.df$variable)){
Class_val[[i]] <- bean_
Variable_val[[i]] <- var_
Skewness_val[[i]] <- Skewness.fun(subset(SeedDataInitial.df, Class == bean_)[,var_])
Kurtosis_val[[i]] <- Kurtosis.fun(subset(SeedDataInitial.df, Class == bean_)[,var_])
i <- i + 1
}
}
#
# beanSkewnessKurtosis.df <- data.frame(Class = unlist(Class_val),
#                                       Variable = unlist(Variable_val),
#                                       Skewness = unlist(Skewness_val),
#                                       Kurtosis = unlist(Kurtosis_val)
#                                       )
# print(beanSkewnessKurtosis.df)
# Run function to remove outliers
SeedDataNoOutliers.df <- remove_outliers_beans_dataset(SeedDataInitial.df)
#Run the function to remove outliers once more as 2 outliers were found after the first run
SeedDataNoOutliers.df <- remove_outliers_beans_dataset(SeedDataNoOutliers.df)
# View bean counts per category
print(SeedCountsNoOutliers <- SeedDataNoOutliers.df %>%
group_by(Class) %>%
summarize(Freq=n()))
#Convert data to long format
SeedDataLong.df <- SeedDataNoOutliers.df %>%                                   # Apply pivot_longer function
pivot_longer(c("Area","Perimeter", "MajorAxisLength", "MinorAxisLength", "Eccentricity", "ConvexArea", "Extent"), names_to = "variable")
#Plots
#Box plots of the dataset
for (var_ in unique(SeedDataLong.df$variable)) {
plotValue <- SeedDataLong.df %>%
filter(variable %in% var_) %>%
ggplot(aes(x=Class, y=value, fill=Class)) +
geom_boxplot() +
stat_boxplot(geom ='errorbar', width = 0.6) +
geom_jitter(width=0.1,alpha=0.2) +
labs(y=var_)
print(plotValue)
}
#Histograms of the dataset
for (var_ in unique(SeedDataLong.df$variable)) {
plotValue2 <- SeedDataLong.df %>%
filter(variable %in% var_) %>%
ggplot( aes(x=value, color=Class, fill=Class)) +
geom_histogram(alpha=0.6) +
scale_fill_viridis(discrete=TRUE) +
scale_color_viridis(discrete=TRUE) +
# theme_ipsum() +
theme(
legend.position="none",
panel.spacing = unit(0.1, "lines"),
axis.text.x = element_text(angle=45)
) +
facet_wrap(~Class)+
labs(subtitle=var_)
print(plotValue2)
}
SeedDataNoOutliers.df <- SeedDataNoOutliers.df[2:9]
maxObsTarget <- max(SeedCountsNoOutliers$Freq)
tempFreqCount = 0
#Loop through the various classes in the dataset
for (class in SeedCountsNoOutliers$Class)
{
# check if a class is less than the observations target of the highest observation class in the dataset
tempFreqCount = subset(SeedCountsNoOutliers, (Class == class), select = Freq)
if (tempFreqCount < maxObsTarget){
# if there aren't enough observations, add some via random walk oversampling
resample.df = sample_n(subset(SeedDataNoOutliers.df, (Class == class)), size = as.integer(maxObsTarget-tempFreqCount), replace = TRUE)
SeedDataNoOutliers.df<-union_all(resample.df,SeedDataNoOutliers.df)
}
}
print(SeedDataNoOutliers.df %>%
group_by(Class) %>%
summarize(Freq=n()))
#Update Dataset to have a numeric version of response variable via the  beanKey.df dataframe
SeedDataNumeric.df <- convert.class.to.classID(SeedDataNoOutliers.df, BeanKey.df)
# Split dataset 80/20
set.seed (10)
rows.trn=sort(sample(1:length(SeedDataNumeric.df[,1]), size = length(SeedDataNumeric.df[,1])*.8, replace = F))
trn.dat=SeedDataNumeric.df[rows.trn,]
tst.dat=SeedDataNumeric.df[-rows.trn,]
# Normalize the data
#Store normalization parameters of mean and standard deviation for use on the test dataset later
NormalizationParams <- data.frame(mean = apply(trn.dat[,1:7],2,mean), sd = apply(trn.dat[,1:7],2,sd))
#Make copy of unnormalized training dataset in case needed later
trn.norm.dat<-trn.dat
#Normalize via the newly created Z-score function
trn.norm.dat$Area <- ZScore.Solve(trn.dat$Area)
trn.norm.dat$Perimeter <- ZScore.Solve(trn.dat$Perimeter)
trn.norm.dat$MajorAxisLength <- ZScore.Solve(trn.dat$MajorAxisLength)
trn.norm.dat$MinorAxisLength <- ZScore.Solve(trn.dat$MinorAxisLength)
trn.norm.dat$Eccentricity <- ZScore.Solve(trn.dat$Eccentricity)
trn.norm.dat$ConvexArea <- ZScore.Solve(trn.dat$ConvexArea)
trn.norm.dat$Extent <- ZScore.Solve(trn.dat$Extent)
#Make copy of unnormalized test dataset in case needed later
tst.norm.dat<-tst.dat
#Normalize the test dataset, utilizing the parameters obtained from the training dataset
tst.norm.dat$Area <- ZScore.Solve(tst.dat$Area, NormalizationParams["Area","mean"], NormalizationParams["Area","sd"])
tst.norm.dat$Perimeter <- ZScore.Solve(tst.dat$Perimeter, NormalizationParams["Perimeter","mean"], NormalizationParams["Perimeter","sd"])
tst.norm.dat$MajorAxisLength <- ZScore.Solve(tst.dat$MajorAxisLength, NormalizationParams["MajorAxisLength","mean"], NormalizationParams["MajorAxisLength","sd"])
tst.norm.dat$MinorAxisLength <- ZScore.Solve(tst.dat$MinorAxisLength, NormalizationParams["MinorAxisLength","mean"], NormalizationParams["MinorAxisLength","sd"])
tst.norm.dat$Eccentricity <- ZScore.Solve(tst.dat$Eccentricity, NormalizationParams["Eccentricity","mean"], NormalizationParams["Eccentricity","sd"])
tst.norm.dat$ConvexArea <- ZScore.Solve(tst.dat$ConvexArea, NormalizationParams["ConvexArea","mean"], NormalizationParams["ConvexArea","sd"])
tst.norm.dat$Extent <- ZScore.Solve(tst.dat$Extent, NormalizationParams["Extent","mean"], NormalizationParams["Extent","sd"])
#Check for any missing values in the datasets
print(paste("Missing values in training dataset: ",sum (is.na( trn.norm.dat ))))
print(paste("Missing values in test dataset: ",sum (is.na( tst.norm.dat ))))
# Run best subset selection
regfit.full<-regsubsets(ClassID ~.,data=trn.norm.dat,nvmax = 7)
reg.summary <- summary(regfit.full)
#Display the results and highlight the row with the highest adjusted R^2 value
FeatureSelectionResults <- cbind(Num_Features = row.names(reg.summary$which), reg.summary$which,'AdjRSquared' = reg.summary$adjr2)
datatable(FeatureSelectionResults[,-2]) %>% formatStyle(
'AdjRSquared',
target = 'row',
backgroundColor = styleEqual(
max(FeatureSelectionResults[,'AdjRSquared']), c('lightgreen')
)
)
formulaVar <- as.formula("ClassID ~ Area + Eccentricity + Perimeter + ConvexArea + Extent")
formulaVarOrig <- as.formula("ClassID ~ Area + Eccentricity  + ConvexArea + Extent")
defaultW <- getOption("warn")
options(warn = -1)
#Found compatible loocv code here; https://www.r-bloggers.com/2015/09/predicting-creditability-using-logistic-regression-in-r-cross-validating-the-classifier-part-2-2/
#Run multinomial logistic regression model with loocv
acc <- NULL
resultsLogModel <- factor(rep(1,length(trn.norm.dat)),levels=seq(1,6))
for(i in 1:nrow(trn.norm.dat))
{
# Train-test splitting
train <- trn.norm.dat[-i,]
validation <- trn.norm.dat[i,]
# Fitting
LogModel <- multinom(formulaVar, data=train, trace=FALSE)
# Predict results
resultsLogModel[i] <- predict(LogModel,subset(validation,select=c(1:7)), type="class")
}
# Capture and print results
correctRes <- factor(trn.norm.dat$ClassID,levels = seq(1,6))
correctResForCompare <- data.frame(ClassID = correctRes)
resultsLogModelForCompare <- data.frame(ClassID = resultsLogModel)
print("weight")
print(LogBeanWeightDiff <- compare.Bean.Weight(correctResForCompare, resultsLogModelForCompare, BeanKey.df))
print("cost")
print(LogBeanCostDiff <- compare.Bean.Cost(correctResForCompare, resultsLogModelForCompare, BeanKey.df))
print(LogConfusionMatrix <- confusionMatrix(data = convert.classID.to.class.2.factor(resultsLogModel), reference = convert.classID.to.class.2.factor(correctRes)))
#Run Linear Discriminant Analysis with loocv
acc <- NULL
resultsLdaModel <- factor(rep(1,length(trn.norm.dat)),levels=seq(1,6))
for(i in 1:nrow(trn.norm.dat))
{
# Train-test splitting
train <- trn.norm.dat[-i,]
validation <- trn.norm.dat[i,]
# Fitting
LdaModel <- lda(formulaVar, data=train)
# Predict results
resultsLdaModel[i] <- predict(LdaModel,subset(validation,select=c(1:7)), type="class")$class
}
#Capture and print results
correctRes <- factor(trn.norm.dat$ClassID,levels = seq(1,6))
correctResForCompare <- data.frame(ClassID = correctRes)
resultsLdaModelForCompare <- data.frame(ClassID = resultsLdaModel)
print("weight")
print(LdaBeanWeightDiff <- compare.Bean.Weight(correctResForCompare, resultsLdaModelForCompare, BeanKey.df))
print("cost")
print(LdaBeanCostDiff <- compare.Bean.Cost(correctResForCompare, resultsLdaModelForCompare, BeanKey.df))
print(LdaConfusionMatrix <- confusionMatrix(data = convert.classID.to.class.2.factor(resultsLdaModel), reference = convert.classID.to.class.2.factor(correctRes)))
#Run K Nearest Neighbors with loocv
acc <- NULL
resultsKnnModel <- factor(rep(1,length(trn.norm.dat)),levels=seq(1,6))
columnsKnn <- c("Area", "Perimeter","Eccentricity","ConvexArea","Extent", "ClassID")
for(i in 1:nrow(trn.norm.dat))
{
# Train-test splitting
train <- trn.norm.dat[-i,columnsKnn]
validation <- trn.norm.dat[i,columnsKnn]
# Fitting
resultsKnnModel[i] <- knn(train[,1:5],validation[,1:5],train[,6], k=30)
}
#Capture and print results
correctRes <- factor(trn.norm.dat$ClassID,levels = seq(1,6))
correctResForCompare <- data.frame(ClassID = correctRes)
resultsKnnModelForCompare <- data.frame(ClassID = resultsKnnModel)
print("weight")
print(KnnBeanWeightDiff <- compare.Bean.Weight(correctResForCompare, resultsKnnModelForCompare, BeanKey.df))
print("cost")
print(KnnBeanCostDiff <- compare.Bean.Cost(correctResForCompare, resultsKnnModelForCompare, BeanKey.df))
print(KnnConfusionMatrix <- confusionMatrix(data = convert.classID.to.class.2.factor(resultsKnnModel), reference = convert.classID.to.class.2.factor(correctRes)))
#Run Quadratic Discriminant Analysis with loocv
acc <- NULL
resultsQdaModel <- factor(rep(1,length(trn.norm.dat)),levels=seq(1,6))
for(i in 1:nrow(trn.norm.dat))
{
# Train-test splitting
train <- trn.norm.dat[-i,]
validation <- trn.norm.dat[i,]
# Fitting
QdaModel <- qda(formulaVar, data=train)
# Predict results
resultsQdaModel[i] <- predict(QdaModel,subset(validation,select=c(1:7)), type="class")$class
}
#Capture and print results
correctRes <- factor(trn.norm.dat$ClassID,levels = seq(1,6))
correctResForCompare <- data.frame(ClassID = correctRes)
resultsQdaModelForCompare <- data.frame(ClassID = resultsQdaModel)
print("weight")
print(QdaBeanWeightDiff <- compare.Bean.Weight(correctResForCompare, resultsQdaModelForCompare, BeanKey.df))
print("cost")
print(QdaBeanCostDiff <- compare.Bean.Cost(correctResForCompare, resultsQdaModelForCompare, BeanKey.df))
print(QdaConfusionMatrix <- confusionMatrix(data = convert.classID.to.class.2.factor(resultsQdaModel), reference = convert.classID.to.class.2.factor(correctRes)))
#Run Naive Bayes model with loocv
acc <- NULL
resultsNbModel <- factor(rep(1,length(trn.norm.dat)),levels=seq(1,6))
for(i in 1:nrow(trn.norm.dat))
{
# Train-test splitting
train <- trn.norm.dat[-i,]
validation <- trn.norm.dat[i,]
# Fitting
NbModel <- naiveBayes(formulaVar, data=train)
# Predict results
resultsNbModel[i] <- predict(NbModel,subset(validation,select=c(1:7)), type="class")
}
#Capture and print results
correctRes <- factor(trn.norm.dat$ClassID,levels = seq(1,6))
correctResForCompare <- data.frame(ClassID = correctRes)
resultsNbModelForCompare <- data.frame(ClassID = resultsNbModel)
print("weight")
print(NbBeanWeightDiff <- compare.Bean.Weight(correctResForCompare, resultsNbModelForCompare, BeanKey.df))
print("cost")
print(NbBeanCostDiff <- compare.Bean.Cost(correctResForCompare, resultsNbModelForCompare, BeanKey.df))
print(NbConfusionMatrix <- confusionMatrix(data = convert.classID.to.class.2.factor(resultsNbModel), reference = convert.classID.to.class.2.factor(correctRes)))
options(warn = defaultW)
# Multinomial Logistic Regression and Linear Discriminant Analysis
# #Setup for KNN Classifier
# IndependentVars <- c('Area','Eccentricity','ConvexArea','Extent')
# Predict results
resultsFinalModelLog <- predict(LogModel,subset(tst.norm.dat,select=c(1:7)), type="class")
resultsFinalModelQDA <- predict(LdaModel,subset(tst.norm.dat,select=c(1:7)), type="class")$class
correctRes <- factor(tst.norm.dat$ClassID,levels = seq(1,6))
#Capture and print results
correctResForCompare <- data.frame(ClassID = correctRes)
resultsLogModelForCompare <- data.frame(ClassID = resultsFinalModelLog)
resultsLdaModelForCompare <- data.frame(ClassID = resultsFinalModelQDA)
print("Multinomial Logistic Regression weight")
print(FinalLogBeanWeightDiff <- compare.Bean.Weight(correctResForCompare, resultsLogModelForCompare, BeanKey.df))
print("Multinomial Logistic Regression cost")
print(FinalLogBeanCostDiff <- compare.Bean.Cost(correctResForCompare, resultsLogModelForCompare, BeanKey.df))
print("Multinomial Logistic Regression")
print(FinalLogConfusion <- confusionMatrix(data = convert.classID.to.class.2.factor(resultsLogModelForCompare), reference = convert.classID.to.class.2.factor(correctRes)))
print("Linear Discriminant Analysis weight")
print(FinalLdaBeanWeightDiff <- compare.Bean.Weight(correctResForCompare, resultsLdaModelForCompare, BeanKey.df))
print("Linear Discriminant Analysis cost")
print(FinalLdaBeanCostDiff <- compare.Bean.Cost(correctResForCompare, resultsLdaModelForCompare, BeanKey.df))
print("Linear Discriminant Analysis")
print(FinalLdaConfusion <- confusionMatrix(data = convert.classID.to.class.2.factor(resultsLdaModelForCompare), reference = convert.classID.to.class.2.factor(correctRes)))
#Suppress warnings
options(warn=-1)
#Results Table of final model results with a highlight on the chosen model
Model_Name <- c('Multinomial Logistic Regression', 'Linear Discriminant Analysis', 'K-Nearest Neighbors', 'Quadratic Discriminant Analysis', 'Naive Bayes Analysis')
Accuracy_Rates <- c(LogConfusionMatrix$overall['Accuracy'],LdaConfusionMatrix$overall['Accuracy'],KnnConfusionMatrix$overall['Accuracy'],QdaConfusionMatrix$overall['Accuracy'],NbConfusionMatrix$overall['Accuracy'])
Bean_Weights <- c(LogBeanWeightDiff,LdaBeanWeightDiff,KnnBeanWeightDiff,QdaBeanWeightDiff,NbBeanWeightDiff)
Bean_Costs <- c(LogBeanCostDiff,LdaBeanCostDiff,KnnBeanCostDiff,QdaBeanCostDiff,NbBeanCostDiff)
# Join the variables to create a data frame
ValidationResultsDisplay.df <- data.frame(ModelID = seq(1,5),Model_Name, Accuracy = round(Accuracy_Rates,4), Weight_Differential = round(Bean_Weights,4), Cost_Differential =round(Bean_Costs,4))
#Utilize datatable to show results and highlight rows of models with the highest accuracy and lowest weight and cost differentials
datatable(ValidationResultsDisplay.df) %>% formatStyle(
c('Accuracy','Weight_Differential','Cost_Differential'),
target = 'row',
backgroundColor = styleEqual(
c(max(ValidationResultsDisplay.df[,'Accuracy']),min(ValidationResultsDisplay.df[,'Weight_Differential']),min(ValidationResultsDisplay.df[,'Cost_Differential'])), c('lightgreen','lightgreen','lightgreen')
)
)
Model_Name <- c('Multinomial Logistic Regression', 'Linear Discriminant Analysis')
Accuracy_Rates <- c(FinalLogConfusion$overall['Accuracy'],FinalLdaConfusion$overall['Accuracy'])
Bean_Weights <- c(FinalLogBeanWeightDiff,FinalLdaBeanWeightDiff)
Bean_Costs <- c(FinalLogBeanCostDiff,FinalLdaBeanCostDiff)
# Join the variables to create a data frame
TestResultsDisplay.df <- data.frame(ModelID = seq(1,2),Model_Name, Accuracy = round(Accuracy_Rates,4), Weight_Differential = round(Bean_Weights,4), Cost_Differential =round(Bean_Costs,4))
#Utilize datatable to show results and highlight rows of models with the highest accuracy and lowest weight and cost differentials
datatable(TestResultsDisplay.df)
options(warn=0)
print(SeedDataNoOutliers.df %>%
group_by(Class) %>%
summarize(Freq=n()))
